时间2020-08-20
今天主要做了一道关于链表的题目，这是第一次这么严谨的来刷leetcode的题目。立下一个flag希望每天都能够
做一道题目吧。如果比较宽泛一些的话，一周至少能够做三道题目吧。

今天还配置了vscode，感觉这个编辑器还是很好用的。希望自己能够好好的利用起来吧。
今天还复习了一下C++的基础知识。比如类，以及构造函数和析构函数。
对于类中可以声明一个函数名字以及返回类型，然后再对函数内部的具体实现进行描述。
class A{
public:
    void B();
};

A::B(){
    /*代码的具体实现*/
}

针对于构造函数，在类中和函数名字一样，其在类生成的时候就自动运行了。
A a; 
当声明a为A类的时候，构造函数已经开始运行了。通常构造函数对类进行一些初始化工作。

析构函数，也和类的名字一样但是前面加上一个~这个符号，这个函数在类结束的时候会自动运行，进行一些销毁工作。

今天在代码方面就做了这写工作。


时间20200827
今天做了一道链表反转的题目，复习了头插法。还有或的性质。

时间20200828
今天做了以到链表合并的题目，还是锻炼了递归的方法。还自己设计了一种方法。不过开始报了一个错误就是没有判读空指针，对空指针进行了读取操作。

设计思路，打算明天赶紧把论文的整体写完了。然后，开始着手第二个工作。设计一个关于NLP处理的加速库，使用CUDA/C++进行编程。
这就是我的研究生期间的主要专攻的点。在开发的过程中参考一些caffe的工作可以试着。这样锻炼了自己的开发能力，以及熟悉了NLP的算法流程，
还成熟的和自己的加速研究结合了起来。
不得不说，这个vscode真的好用呀。哈哈哈

时间20200823
坚持了第四天了，今天还是做了一道链表的题目。去除有序表的重复值。设计了一种自己的算法。
不过还是不太会递归的方法做这种题目。导致时间和内存损耗比较大。

今天主要把论文的CRF写完了，不过不是非常的明白。
使用了c++对RNN写了一个简单的extension不过还不是很完善，只写了前向算法，没有后向传播算法。
明天仔细研究一下，完成这个简单的RNN的基础算法，并且和之前的算法进行一个对比。此外，现在使用的乘法
是torch自带的乘法，感觉这样不是很快，明天试着修改一下。然后，完成一个cuda版本的算法，写一个完整的接口算法
这样就可以得到快速的结果了。此外，引入大规模稀疏矩阵的计算，看一下有没有什么优化的方法。好像是有相关的论文可以做一下。

现在来说，整体的研究思路就是整合一个NLP的coder应用和加速结合起来。
初步的规划如下:rnn_base_cpu->rnn_base_gpu->rnn_base_sparse_cpu_gpu->rnn变体
                cnn_nlp版本优化